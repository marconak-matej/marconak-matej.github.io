---
layout: post
title: "[MM’s] Terraform Notes — Kafka as Code with Terraform and Confluent Cloud"
canonical_url: https://marconak-matej.medium.com/mms-terraform-notes-kafka-as-code-with-terraform-and-confluent-cloud-c8c750bdf3c9?source=rss-f1368b7746f7------2
tag:
- kafka
- terraform
- confluent
- programming
---

<h3>[MM’s] Terraform Notes — Kafka as Code with Terraform and Confluent Cloud</h3><h4>Mastering Confluent Cloud resource management with HCL.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1015/1*oz8AWr6IugW0_zKBvMU8Ig.png" /><figcaption>Kafka as Code with Terraform and Confluent Cloud</figcaption></figure><p>Confluent Cloud Kafka is often introduced to teams as a managed service — <em>no servers, no worries</em>. But once you have multiple teams, services and environments, the reality changes fast.</p><p>Manual setup through the Confluent Cloud UI leads to:</p><ul><li>Slightly different topic configs between environments</li><li>Overprivileged service accounts created “just to make it work”</li><li>No reliable audit trail of <em>who changed what and why</em></li></ul><p>At that point, Kafka infrastructure becomes a liability rather than an enabler.</p><h4>⚡ TL;DR (Quick Recap)</h4><ul><li>Kafka infrastructure should be versioned, reviewed and reproducible</li><li>Terraform provides a clean abstraction over Confluent Cloud APIs</li><li>Modular design enables safe reuse across teams and environments</li></ul><h4>Why Kafka as Code Matters Now</h4><p>Modern Kafka usage goes far beyond a handful of topics. Today’s platforms often include:</p><ul><li>Multiple Kafka clusters across regions</li><li>Fine-grained ACLs per service and consumer group</li><li>Strict separation between dev, staging and production</li></ul><p>Without Infrastructure as Code, keeping this consistent is nearly impossible. Terraform turns Kafka into a <strong>declarative system</strong>: the desired state is written once, reviewed in Git and enforced automatically.</p><h4>Designing a Modular Kafka Repository</h4><p>Instead of a single, monolithic Terraform configuration, the repository is split into focused modules. Each module solves one problem well and does so <strong>explicitly</strong>, avoiding hidden defaults.</p><p><strong>Cluster Module (Best-Practice Hardened)</strong></p><p>Clusters are environment-scoped resources and should always be created alongside their Confluent environment. Provider versions are pinned to avoid breaking changes.</p><pre>terraform {<br>  required_providers {<br>    confluent = {<br>      source  = &quot;confluentinc/confluent&quot;<br>      version = &quot;~&gt; 2.57&quot;<br>    }<br>  }<br>}<br><br>resource &quot;confluent_environment&quot; &quot;this&quot; {<br>  display_name = var.environment_name<br>}<br>resource &quot;confluent_kafka_cluster&quot; &quot;this&quot; {<br>  display_name = var.cluster_config.name<br>  availability = var.cluster_config.availability<br>  cloud        = var.cluster_config.cloud_provider<br>  region       = var.cluster_config.region<br>  <br>  dynamic &quot;basic&quot; {<br>    for_each = var.cluster_config.type == &quot;BASIC&quot; ? [1] : []<br>    content {}<br>  }<br><br>  dynamic &quot;standard&quot; {<br>    for_each = var.cluster_config.type == &quot;STANDARD&quot; ? [1] : []<br>    content {}<br>  }<br><br>  dynamic &quot;dedicated&quot; {<br>    for_each = var.cluster_config.type == &quot;DEDICATED&quot; ? [1] : []<br>    content {<br>      cku = var.cluster_config.cku<br>    }<br>  }<br>  environment {<br>    id = confluent_environment.this.id<br>  }<br>}</pre><p><strong>Service Identity Module (Least Privilege by Default)</strong></p><p>Service accounts represent applications — never humans. They are cheap to create and should be scoped tightly.</p><pre>locals {<br>  display_name = &quot;${var.identity.team}-${var.identity.name}&quot;<br>  description  = var.identity.description != &quot;&quot; ? var.identity.description : &quot;Service account for ${var.identity.team}/${var.identity.name}&quot; <br>}<br>resource &quot;confluent_service_account&quot; &quot;this&quot; {<br>  display_name = local.display_name<br>  description  = local.description<br>}</pre><p><strong>Topic Module (Explicit, Protected, Reviewable)</strong></p><p>Kafka topics are part of your public contract. Defaults are dangerous and deletions should be guarded.</p><pre>locals {<br>  topic_name = &quot;${var.team}.${var.name}&quot;<br>}<br>resource &quot;confluent_kafka_topic&quot; &quot;this&quot; {<br>  kafka_cluster {<br>    id = var.cluster.id<br>  }<br>  topic_name       = local.topic_name<br>  partitions_count = var.topic_config.partitions<br>  config = {<br>    &quot;cleanup.policy&quot;      = var.topic_config.cleanup_policy<br>    &quot;retention.ms&quot;        = var.topic_config.retention_ms<br>    &quot;min.insync.replicas&quot; = var.topic_config.min_insync_replicas<br>  }<br>  rest_endpoint = var.cluster.rest_endpoint<br>  credentials {<br>    key    = var.cluster.api_key<br>    secret = var.cluster.api_secret<br>  }<br>  lifecycle {<br>    prevent_destroy = true<br>  }<br>}</pre><p><strong>Topic ACLs (Never Implicit)</strong></p><p>Topics without ACLs are configuration drift waiting to happen. Permissions must be explicit and reviewable.</p><pre>resource &quot;confluent_kafka_acl&quot; &quot;topic_read&quot; {<br>  kafka_cluster {<br>    id = var.cluster.id<br>  }<br>  resource_type = &quot;TOPIC&quot;<br>  resource_name = confluent_kafka_topic.this.topic_name<br>  pattern_type  = &quot;LITERAL&quot;<br>  principal     = &quot;User:${var.reader_service_account_id}&quot;<br>  operation     = &quot;READ&quot;<br>  permission    = &quot;ALLOW&quot;<br>  rest_endpoint = var.cluster.rest_endpoint<br>  credentials {<br>    key    = var.cluster.api_key<br>    secret = var.cluster.api_secret<br>  }<br>}</pre><p><strong>Environment Isolation Without Duplication</strong></p><p>The environments/ directory separates <em>what</em> you deploy from <em>where</em> you deploy it.</p><ul><li>dev/ uses smaller, cost-effective clusters</li><li>prod/ enables high availability and stricter limits</li></ul><p>The same modules are reused everywhere; only variables change. This guarantees parity while allowing intentional differences.</p><h4>The Deployment Workflow</h4><p>Once the modules are in place, Kafka provisioning becomes boring — in the best possible way.</p><pre>terraform init<br>terraform plan -out=dev.tfplan<br>terraform apply dev.tfplan</pre><p>No hidden steps. No UI clicks. The plan shows exactly what will change before anything happens.</p><p>In practice, these commands are wrapped in CI pipelines, ensuring that Kafka changes follow the same deployment standards as application code.</p><h4>What This Solves in Real Teams</h4><p>Adopting Kafka as Code immediately improves day-to-day operations:</p><ul><li><strong>Consistency:</strong> dev, staging and prod are structurally identical</li><li><strong>Security:</strong> Permissions are explicit and reviewable</li><li><strong>Speed:</strong> New services get Kafka resources in minutes</li><li><strong>Confidence:</strong> Rollbacks are possible because changes are tracked</li></ul><p>New engineers can understand the Kafka setup by reading Terraform.</p><h4>What Confluent Docs Don’t Tell You</h4><p>The Confluent Terraform Provider is powerful — but there are several realities you only discover after running Kafka in production.</p><p><strong>ACLs are not optional.</strong> Creating topics without explicit ACLs works — until a new service appears and permissions are patched manually. If ACLs are not in Terraform, they <em>will</em> drift.</p><p><strong>Defaults are dangerous.</strong> Broker defaults differ between cluster types and evolve over time. If retention or cleanup policies matter, declare them explicitly or expect surprises.</p><p><strong>Terraform deletions are risky.</strong> A terraform destroy on a Kafka topic is irreversible. prevent_destroy is not paranoia — it’s operational hygiene.</p><p><strong>Workspaces often don’t map to Kafka reality.</strong> Kafka environments are not interchangeable. Separate state files beat clever abstractions every time.</p><h4>Lessons Learned</h4><ul><li>Never commit API keys — inject them via CI, Terraform Cloud or Vault</li><li>Always use remote state for Kafka infrastructure</li><li>Keep modules opinionated and small</li><li>Start simple, evolve only when real constraints appear</li></ul><h4>Final Takeaways</h4><p>Kafka as Code is not about Terraform — it’s about <strong>control</strong>.</p><p>By modeling Kafka infrastructure declaratively, you replace manual effort with repeatable systems. Confluent Cloud handles the operational heavy lifting; Terraform ensures that <em>your intent</em> is clearly expressed and consistently applied.</p><p>Start with one environment. One cluster. One topic.</p><p>You can find all the code on <a href="https://github.com/marconak-matej/confluent-cloud-ops">GitHub</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c8c750bdf3c9" width="1" height="1" alt="">
